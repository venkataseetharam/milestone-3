{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8d769ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import activations,optimizers,losses\n",
    "from transformers import DistilBertTokenizer,TFDistilBertForSequenceClassification\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "73864281",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='distilbert-base-uncased'#using distilbert model\n",
    "max_len=512#defining max length of the words to be taken\n",
    "tokenizer=DistilBertTokenizer.from_pretrained(model_name)#intializing distilbert tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3321942a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset hupd (C:/Users/91800/.cache/huggingface/datasets/HUPD___hupd/sample-23bcfec45c886e8c/0.0.0/6920d2def8fd7767046c0470603357f76866e5a09c97e19571896bfdca521142)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af0f616268734cd8bf4fd87fb2fc64e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train=[]\n",
    "# importing datasets and splitiing into training and validation\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset_dict = load_dataset('HUPD/hupd',\n",
    "    name='sample',\n",
    "    data_files=\"https://huggingface.co/datasets/HUPD/hupd/blob/main/hupd_metadata_2022-02-22.feather\", \n",
    "    icpr_label=None,\n",
    "    train_filing_start_date='2016-01-01',\n",
    "    train_filing_end_date='2016-01-21',\n",
    "    val_filing_start_date='2016-01-22',\n",
    "    val_filing_end_date='2016-01-31',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "abfd0308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding the decisions field in patents dataset because they are in the strings\n",
    "decision_to_str = {'REJECTED': 0, 'ACCEPTED': 1, 'PENDING': 2, 'CONT-REJECTED': 3, 'CONT-ACCEPTED': 4, 'CONT-PENDING': 5}\n",
    "#function to encode\n",
    "def map_decision_to_string(example):\n",
    "    return {'decision': decision_to_str[example['decision']]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0770c0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\91800\\.cache\\huggingface\\datasets\\HUPD___hupd\\sample-23bcfec45c886e8c\\0.0.0\\6920d2def8fd7767046c0470603357f76866e5a09c97e19571896bfdca521142\\cache-93d3351fa3e3a5e1.arrow\n",
      "Loading cached processed dataset at C:\\Users\\91800\\.cache\\huggingface\\datasets\\HUPD___hupd\\sample-23bcfec45c886e8c\\0.0.0\\6920d2def8fd7767046c0470603357f76866e5a09c97e19571896bfdca521142\\cache-0da8d16936d2b5f5.arrow\n",
      "Loading cached processed dataset at C:\\Users\\91800\\.cache\\huggingface\\datasets\\HUPD___hupd\\sample-23bcfec45c886e8c\\0.0.0\\6920d2def8fd7767046c0470603357f76866e5a09c97e19571896bfdca521142\\cache-8e7c04f871a5f3dd.arrow\n",
      "Loading cached processed dataset at C:\\Users\\91800\\.cache\\huggingface\\datasets\\HUPD___hupd\\sample-23bcfec45c886e8c\\0.0.0\\6920d2def8fd7767046c0470603357f76866e5a09c97e19571896bfdca521142\\cache-47be42eb7b704c2e.arrow\n"
     ]
    }
   ],
   "source": [
    "#mapping the encoding function to training and validation data\n",
    "for name in ['train', 'validation']:\n",
    "        dataset_dict[name] = dataset_dict[name].map(map_decision_to_string)\n",
    "        # Remove the pending and CONT-patent applications\n",
    "        dataset_dict[name] = dataset_dict[name].filter(lambda e: e['decision'] <= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b6ab981c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collecting the absatact values of the training dataset\n",
    "x_train=dataset_dict['train']['abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3c64501a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collecting the absatact values of the validation dataset\n",
    "x_val=dataset_dict['validation']['abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "533c2ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for creating masks and input_ids and this function takes input and type of tokenizer we declared and returns output encodings \n",
    "def construct_encodings(x,tk,max_len,truncation=True,padding=True):\n",
    "    return tk(x,max_length=max_len,truncation=truncation,padding=padding)\n",
    "encodings=construct_encodings(x_train,tokenizer,max_len=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "793873d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking decisions of the patent into seperate list\n",
    "y_train=[]\n",
    "y_train=dataset_dict['train']['decision']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eda866ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking decisions of the patent into seperate list\n",
    "y_val=[]\n",
    "y_val=dataset_dict['validation']['decision']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "087a4ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for making the data to fit into the model.Here in tensorflow to fit the model for transformers we have to give input_ids and mask to tensor slices\n",
    "def construct_tfdataset(encodings, y=None):\n",
    "    if y:\n",
    "        return tf.data.Dataset.from_tensor_slices((dict(encodings),y))\n",
    "    else:\n",
    "        # this case is used when making predictions on unseen samples after training\n",
    "        return tf.data.Dataset.from_tensor_slices(dict(encodings))\n",
    "    \n",
    "tfdataset = construct_tfdataset(encodings, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b60a14e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=8 #declaring batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "771bdd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tfdataset = tfdataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dd1ea331",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'activation_13', 'vocab_layer_norm', 'vocab_transform']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_179', 'pre_classifier', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1090/1090 [==============================] - 8604s 8s/step - loss: 0.5312 - accuracy: 0.7948\n",
      "Epoch 2/2\n",
      "1090/1090 [==============================] - 8050s 7s/step - loss: 0.5117 - accuracy: 0.7964\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fabcc2d4f0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_EPOCHS = 2  #number of epoch to train the data\n",
    "\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained(model_name)#preparing the model\n",
    "optimizer = optimizers.Adam(learning_rate=3e-3)#Instantiating the optimizer\n",
    "loss = losses.SparseCategoricalCrossentropy(from_logits=True)#declaring the loss\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])#compiling the model\n",
    "\n",
    "model.fit(tfdataset, batch_size=BATCH_SIZE, epochs=N_EPOCHS)#fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6c779634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model trained weights to be used in future purpose\n",
    "model.save_pretrained('./model/clf')\n",
    "with open('./model/info.pkl', 'wb') as f:\n",
    "    pickle.dump((model_name, max_len), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5a2f3fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings_val=construct_encodings(x_val,tokenizer,max_len=max_len)#encoding validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b3e40aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdataset_test = construct_tfdataset(encodings_val,y_val)#encoding validation data\n",
    "tfdataset_test = tfdataset_test.batch(BATCH_SIZE)#splitting validation data into batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e3cee96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "611/611 [==============================] - 1376s 2s/step - loss: 0.5055 - accuracy: 0.7964\n",
      "{'loss': 0.5054701566696167, 'accuracy': 0.7964402437210083}\n"
     ]
    }
   ],
   "source": [
    "#after training the model we are evaluating the accuracy on the validation data\n",
    "benchmarks = model.evaluate(tfdataset_test, return_dict=True, batch_size=BATCH_SIZE)\n",
    "print(benchmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ace7cefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a function written to predict probabilities of the occurence for the new data point\n",
    "def create_predictor(model, model_name, max_len):\n",
    "  tkzr = DistilBertTokenizer.from_pretrained(model_name)\n",
    "  def predict_proba(text):\n",
    "      x = [text]\n",
    "\n",
    "      encodings = construct_encodings(x, tkzr, max_len=max_len)\n",
    "      tfdataset = construct_tfdataset(encodings)\n",
    "      tfdataset = tfdataset.batch(1)\n",
    "\n",
    "      preds = model.predict(tfdataset).logits\n",
    "      preds = activations.softmax(tf.convert_to_tensor(preds)).numpy()\n",
    "      return preds[0][0]\n",
    "    \n",
    "  return predict_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8f8a7324",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample data point\n",
    "x=(\"Embodiments of the invention proving a tag based on inherent disorder during a manufacturing process. The method includes using a first reader to take a first reading of an inherent disorder feature of the tag, and using a second reader to take a second reading of the inherent disorder feature of the tag. The method further includes matching the first reading with the second reading, and determining one or more acceptance criteria, wherein at least one of the acceptance criteria is based on whether the first reading and the second reading match within a predetermined threshold. If the acceptance criteria are met, then the tag is accepted, and a fingerprint for the tag is recorded. The invention further provides a method of testing and characterizing a reader of inherent disorder tags during a manufacturing process. The method includes taking a reading of a known inherent disorder tag, using the reading to measure a characteristic of the reader, and storing the measured characteristic for use when reading inherent disorder tags.', 'claims': '1. A method comprising: using a first reader to take a first reading of an inherent disorder feature of a tag; using at least a second reader to take at least a second reading of the inherent disorder feature of the tag; matching the first reading with at least the second reading; determining one or more acceptance criteria, wherein at least one of the acceptance criteria is based on whether the first reading and the second reading match within a predetermined threshold; accepting the tag if the acceptance criteria are met; and recording a fingerprint for the tag if the tag was accepted. 2. The method of claim 1, wherein determining one or more acceptance criteria further comprises: determining an acceptance criterion based on an individual reading. 3. The method of claim 2, wherein determining an acceptance criterion based on an individual reading comprises determining an acceptance criterion based on a strength of a signal in at least one of the first reading and the second reading. 4. The method of claim 2, wherein determining an acceptance criterion based on an individual reading comprises determining an acceptance criterion based on a complexity of a signal in at least one of the first reading and the second reading. 5. The method of claim 1, further comprising: rejecting the tag if it is not accepted. 6. The method of claim 5, wherein rejecting the tag comprises removing the tag without stopping the flow of production. 7. The method of claim 6, wherein removing the tag comprises one or more of marking the tag as rejected, cutting out the tag, punching out the tag, and removing a tag using a suction method. 8. The method of claim 5, wherein rejecting the tag further comprises noting the rejected tag in a database. 9. The method of claim 1, further comprising: using at least a third reader to take at least a third reading of the inherent disorder feature of the tag if the acceptance criteria are not met; matching the third reading with the first reading and the second reading; determining one or more further acceptance criteria, wherein at least one of the further acceptance criteria is based on whether the first reading and the third reading match within the predetermined threshold or whether the second reading and the third reading match within the predetermined threshold; and accepting the tag if the further acceptance criteria are met; and if the tag is accepted, recording a fingerprint for the tag based on the first reading if the first reading and the third reading match within the predetermined threshold or based on the second reading if the second reading\")\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "12b94f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ./model/clf were not used when initializing TFDistilBertForSequenceClassification: ['dropout_179']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at ./model/clf and are newly initialized: ['dropout_199']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "0.21113838\n"
     ]
    }
   ],
   "source": [
    "#For the sample data point based on the training weights we are calculating the probability of acceptance of patent.\n",
    "new_model = TFDistilBertForSequenceClassification.from_pretrained('./model/clf')\n",
    "model_name, max_len = pickle.load(open('./model/info.pkl', 'rb'))\n",
    "\n",
    "clf = create_predictor(new_model, model_name, max_len)\n",
    "print(clf(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4703cfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
